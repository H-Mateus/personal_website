{
  "hash": "259f5de58c7b884f603052ad2157ac21",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Testing Normality in R\ndate: '2022-10-21'\ncategories:\n  - R\n  - Demo\n  - Statistics\ndraft: false\ndescription: A quick demo of testing normality in R\n#location: Virtual\n#subtitle: Testing\nimage: featured.png\n---\n\nMany a statistical test assume the data follows a normal (AKA [Gaussian](https://en.wikipedia.org/wiki/Normal_distribution)) distribution.\nThese tests are called parametric tests as their validity depends on the distribution of the data.\n\nIt is therefore important to access the normality of data if we wish to use parametric tests.\n\nNote that the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) tells us that with a large enough sample size (~>40), data will trend to be normally distributed, so one can often get away with using parametric tests in these cases.\n\n# Setup\n\nLet's start by loading our packages.\nWe'll use `ggpubr` as it makes nice plots, and has a handy function for [Q-Q plots](https://data.library.virginia.edu/understanding-q-q-plots/), and `cowplot` for arranging plots into grids.\n\n`rstatix` is for pipe-friendly versions of the base R stat functions, and `tidyverse` for data manipulation.\n\nNote that `librarian` is a neat package for automatically installing and loading packages!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(librarian)\npkg <- c(\"ggpubr\", \"cowplot\", \"tidyverse\", \"rstatix\")\nshelf(pkg)\nrm(pkg)\n```\n:::\n\n\n# Visualising the data\n\nOne the first and best places to start when assessing normality is to simply plot the data!\n\nWe can use the base R `rnorm` function to generate a random set of normal distributed values for demonstration:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## set a seed so our \"random\" numbers don't change\nset.seed(123)\n## generate a random normal distribution\nnormal_distribution <- rnorm(1000)\n## make a density plot\np1 <- ggdensity(\n  normal_distribution,\n  fill = \"lightgray\",\n  title = \"Normally distributed data\"\n)\n## make a Q-Q plot\np2 <- ggqqplot(normal_distribution, title = \"Normally distributed data\")\n## arrange plots into a grid\nplot_grid(p1, p2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\nNotice the classic bell-shaped curve in the density plot and the nice straight line in the Q-Q plot?\nThese are the hallmarks of a normally distributed dataset.\n\nNow let's see what a skewed distribution looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskewed_data <- rbeta(1000, 10, 2)\np1 <- ggdensity(skewed_data, fill = \"lightgray\", title = \"Skewed data\")\np2 <- ggqqplot(skewed_data, title = \"Skewed data\")\nplot_grid(p1, p2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nSee how the density plot \"leans over\" to the right and the points in the Q-Q plot curve away from our theoretical gray lines?\n\nQ-Q plots (or quantile-quantile plots) draw the correlation between a given sample and the normal distribution. \nA 45-degree reference line is also plotted. \nIn a QQ plot, each observation is plotted as a single dot. \nIf the data are normal, the dots should form a straight line.\n\n# Shapiro-Wilk's normality test\n\nIn cases of strong skew, like the data above, visual inspection if fine, but in cases of weaker skew it can be more ambiguous.\n\nSo another method is to use a significance test comparing the sample distribution to a normal one in order to ascertain any deviation from normality.\n\nThere are a few methods to evaluate normality, including the [Kolmogorov-Smirnov (K-S) normality test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) and the [Shapiro-Wilk's](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) test.\n\nShapiro-Wilk's method is better powered than K-S and so tends to be the recommend approach.\n\nLike most common statistical tests, R has handy included function for computing them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(normal_distribution)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  normal_distribution\nW = 0.9983761923, p-value = 0.476468592\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(skewed_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  skewed_data\nW = 0.9487249143, p-value < 2.220446e-16\n```\n\n\n:::\n:::\n\n\nAs we can see, the normally distributed data has a high p-value, tell us it is likely to be normally distributed (shocking I know!).\nBy contrast, the skewed data has a very low p-value, indicating it is unlikely to be normally distributed (also shocking!).\n\n# Comparing groups\n\nIf we have two independent groups, we'd want to check their normality separately.\nIf the underlying distributions are truly different, then they wouldn't be normally distributed if measured together (it'd be bimodal in the case of 2 groups).\n\nThis is nice and easy to do with long-format data.\nWe'll use the built-in `ToothGrowth` dataset to demonstrate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## display some random rows of ToothGrowth\nToothGrowth %>%\n  sample_n_by(supp, dose, size = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n    len supp   dose\n  <dbl> <fct> <dbl>\n1  21.5 OJ      0.5\n2  23.6 OJ      1  \n3  26.4 OJ      2  \n4   5.2 VC      0.5\n5  16.5 VC      1  \n6  23.6 VC      2  \n```\n\n\n:::\n\n```{.r .cell-code}\n## check normality for long-format grouped data\np1 <- ggdensity(ToothGrowth, x = \"len\", color = \"supp\")\np2 <- ggqqplot(ToothGrowth, x = \"len\", color = \"supp\")\nplot_grid(p1, p2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nToothGrowth %>%\n  group_by(supp) %>%\n  shapiro_test(len)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  supp  variable statistic      p\n  <fct> <chr>        <dbl>  <dbl>\n1 OJ    len          0.918 0.0236\n2 VC    len          0.966 0.428 \n```\n\n\n:::\n:::\n\n\nTo apply Shapiro-Wilk's across columns is easy enough.\nWe'll use another built-in dataset, `iris`, to demonstrate with wide-format data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## display some rows of iris\niris %>%\n  sample_n_by(Species, size = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n         <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n1          4.7         3.2          1.6         0.2 setosa    \n2          5.5         2.3          4           1.3 versicolor\n3          7.2         3.6          6.1         2.5 virginica \n```\n\n\n:::\n\n```{.r .cell-code}\n## with column names\niris %>%\n  group_by(Species) %>%\n  shapiro_test(Sepal.Length, Petal.Length)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n  Species    variable     statistic      p\n  <fct>      <chr>            <dbl>  <dbl>\n1 setosa     Petal.Length     0.955 0.0548\n2 setosa     Sepal.Length     0.978 0.460 \n3 versicolor Petal.Length     0.966 0.158 \n4 versicolor Sepal.Length     0.978 0.465 \n5 virginica  Petal.Length     0.962 0.110 \n6 virginica  Sepal.Length     0.971 0.258 \n```\n\n\n:::\n:::\n\n\nBut for the plots, it's a lot easier to pivot to long format:\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_long <- iris %>%\n  pivot_longer(Sepal.Length:Petal.Width, names_to = \"variable\")\n\nggdensity(iris_long, x = \"value\", color = \"variable\", facet.by = \"Species\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggqqplot(iris_long, x = \"value\", color = \"variable\", facet.by = \"Species\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\nI'd recommend checking out the principals of [tidy data](https://tidyr.tidyverse.org/articles/tidy-data.html) if you're unsure of how best to structure your own data.\n\n# Summary\n\nHopefully this gives you a sense of how to check for normality.\nNote, that even in cases where it is fairly obvious the data is skewed, it can be worth doing a test like Shapiro-Wilk's, if only to provide a formal justification for the non-parametric tests that you subsequently use.\n\nAlso note that the Shapiro-Wilk test become sensitive to small deviations from normality at larger sample sizes (>50), so it's generally good to check the visualisations **and** the test.\n\nFinally, if you have a larger sample size and still in doubt about the normality of your data, you can just use non-parametric stats to be safe.\nYou'll lose a little power, but so long as the sample size is large enough, it's unlikely to be critical. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}