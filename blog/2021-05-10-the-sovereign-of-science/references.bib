@journalArticle{smaldino_natural_2016,
  title = {The natural selection of bad science},
  abstractNote = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
  accessDate = {2021-05-13 07:14:58},
  libraryCatalog = {royalsocietypublishing.org (Atypon)},
  volume = {3},
  publisher = {Royal Society},
  publicationTitle = {Royal Society Open Science},
  date = {2016-00-00 2016},
  year = {2016},
  issue = {9},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.160384},
  key = {E3K88WP5},
  journalAbbreviation = {Royal Society Open Science},
  author = {Smaldino, Paul E. and McElreath, Richard},
  pages = {160384},
  DOI = {10.1098/rsos.160384},
}
@journalArticle{wen_low_2018,
  title = {On the low reproducibility of cancer studies},
  pages = {619-624},
  url = {https://doi.org/10.1093/nsr/nwy021},
  accessDate = {2021-05-10 08:22:09},
  libraryCatalog = {Silverchair},
  volume = {5},
  DOI = {10.1093/nsr/nwy021},
  date = {2018-09-01 September 1, 2018},
  year = {2018},
  issue = {5},
  key = {MY76LB2M},
  journalAbbreviation = {National Science Review},
  publicationTitle = {National Science Review},
  author = {Wen, Haijun and Wang, Hurng-Yi and He, Xionglei and Wu, Chung-I},
  ISSN = {2095-5138},
}
@journalArticle{baker_1500_2016,
  title = {1,500 scientists lift the lid on reproducibility},
  abstractNote = {Survey sheds light on the ‘crisis’ rocking research.},
  extra = {Number: 7604},
  accessDate = {2021-05-10 08:21:18},
  libraryCatalog = {www.nature.com},
  year = {2016},
  volume = {533},
  publisher = {Nature Publishing Group},
  issue = {7604},
  DOI = {10.1038/533452a},
  date = {2016-05-01 2016-05-01},
  language = {en},
  author = {Baker, Monya},
  ISSN = {1476-4687},
  key = {PINAJ9AQ},
  publicationTitle = {Nature},
  pages = {452-454},
  url = {https://www.nature.com/articles/533452a},
  rights = {2016 Nature Publishing Group},
}
@journalArticle{head_extent_2015,
  title = {The Extent and Consequences of P-Hacking in Science},
  abstractNote = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  accessDate = {2021-05-10 10:37:50},
  libraryCatalog = {PLoS Journals},
  year = {2015},
  volume = {13},
  publisher = {Public Library of Science},
  ISSN = {1545-7885},
  publicationTitle = {PLOS Biology},
  date = {2015-03-13 Mar 13, 2015},
  language = {en},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  issue = {3},
  key = {7LK6K6ME},
  journalAbbreviation = {PLOS Biology},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106},
  DOI = {10.1371/journal.pbio.1002106},
  pages = {e1002106},
}
