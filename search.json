[
  {
    "objectID": "talk/2022-08-14-proteomic-and-bioinformatics-analyses-of-plasma-from-sci-neurological-improvers-and-non-improvers/index.html",
    "href": "talk/2022-08-14-proteomic-and-bioinformatics-analyses-of-plasma-from-sci-neurological-improvers-and-non-improvers/index.html",
    "title": "Proteomic and Bioinformatics Analyses of Plasma From Sci Neurological Improvers and Non Improvers",
    "section": "",
    "text": "Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "talk/2020-06-08-joint-cdt-conference-2020/index.html",
    "href": "talk/2020-06-08-joint-cdt-conference-2020/index.html",
    "title": "Joint CDT Conference 2020",
    "section": "",
    "text": "Welcome!\nThis post relates to the poster I presented at the virtual Joint CDT conference of 2020.\n\n\nPoster formatting\nIf anyone is actually reading this I‚Äôm guessing this is what caught your attention.\nThe poster was made in R-Markdown with a great package called posterdown by Brent Thorne and follows the formatting rules outlined by Mike Morrison\nMike explains his rationale in this video, but the goal is to refocus scientific posters to emphasize the key finding of a piece of work in a easily digestible way. This format has a lot less information than the typical wall of text and data found in most scientific posters, but that‚Äôs the whole point!\nMany people won‚Äôt read or remember these kinds of walls of information and frankly, they likely don‚Äôt care about the highly specific details. If someone does want more information they can use the QR code included in the format or ask the poster author.\nTypically this QR code would link to a publication of the work if it‚Äôs available. This is much easier to both make and read the poster, hopefully facilitating more communication of science by making it possible to digest more posters, which is the whole point of a conference.\nThis rest of this post is non-technical explanation of the work my poster presents. We hope to publish this work later this year so if/when it goes up I‚Äôll update this post with a link to that.\n\n\nProteomics in spinal cord injury recovery\nSo the poster presents some ongoing proteomic analysis which investigates recovery following spinal cord injury (SCI).\nPlasma was collected from 17 SCI patients within 2 weeks post-injury and approximately 3 months post-injury. All these patients were AIS grade C, meaning they had ‚Äúmedium‚Äù severity SCI in terms of neurological function loss.\nOf these 17 patients, 10 experienced an AIS grade conversion, meaning they had significant recovery of neurological function. The remaining 7 patients did not experience this level of neurological recovery.\nIn an effort to investigate why some patients recovered much more than others we used the plasma collected to conduct some proteomic experiments.\n\n\nData analysis\nWe choose to do an 4-plex isobaric tag for relative and absolute quantitation (iTRAQ) experiment. This means we‚Äôre comparing the relative levels of proteins in 4 groups, where each group is a pooled sample (from each patients in this case).\nOur groups were as follows:\n\nAIS conversion patients (‚Äúimprovers‚Äù) at 2 weeks\nAIS conversion patients (‚Äúimprovers‚Äù) at 3 months\nAIS stable patients (‚Äúnon-improvers‚Äù) at 2 weeks\nAIS stable patients (‚Äúnon-improvers‚Äù) at 3 months\n\nAs we don‚Äôt have a mass spectrometer for proteomic work at Oswestry, we sent our pooled samples to a lab at St.¬†Andrews. They also did the raw data processing with the mass spec vendor software called ProteinPilot. This is where the data in the poster figure comes from.\nOf the proteins identified in this initial analysis, some of them are associated with liver function, which is of particular interest as our previous work has identified liver function as being predictive of recovery following SCI. This data also shows large differences in the proteomes (the abundance of given proteins) of patients who experience a large improvement as compared to those who do not. The STRING plots indicate that the proteins identified are all related in function, and are largely pathways associated with the immune system.\nHaving said all this, I‚Äôm am generally distrustful of proprietary, closed-source software, as it isn‚Äôt clear what the software actually does to your data, which is not conducive to reproducibility.\n\n\nReproducible research\nReproducible research is important.\nNo really. It matters.\nYou may think this is obvious. Patronising even.\nHowever, even at this early stage in my career, I‚Äôve seen many a scientist who think this only applies to their experiments. Of course, your experiments should have well documented methods that allow others to reproduce your work and hopefully get the same result. But what about your data analysis?\nWhen we use software like Microsoft Excel or SPSS, there is no record of exactly what we‚Äôve done. Sure you can write something like ‚ÄúA T-test was performed‚Äù in your method but there‚Äôs nothing that tells me precisely how you went from the raw data your experiments generate to the graphs and tables I see in your paper/poster/talk.\nThis can be especially problematic in a case like this proteomic work. The software ProteinPilot was create by the vendor who sells the mass spec. It takes the data the mass spec generates and gives nice, easy to work with fold-changes of proteins between our groups. It‚Äôs even generous enough to give the user some options when it process the data.\nBut it isn‚Äôt clear exactly what the software does to the data. There are many important steps in processing this data, in turning m/z values and their intensities into peptides, then proteins, and correcting for false discovery rates. There are many more important steps which could dramatically change your results, and each presents a forest of a thousand paths one could take. But the software just does it all for you. It even uses some fancy proprietary algorithms in it‚Äôs protein identification!\nNow if I wanted to go back 10 years later, and do the same analysis on the same data, would I be able to? Would this software even exist? Would the algorithm have been quietly tweaked in the mean time? How can any academic scrutinize what I‚Äôve done if I have no clue what assumptions this software makes behind the scenes?\nIt‚Äôs for these reasons that I‚Äôm currently looking into redoing the processing of the raw data in R and/or other open-source tools. This will allow me to have a script that I and others can use to properly understand what was done. This is also a big undertaking as I have no formal training in bioinformatics but I want to try regardless. Simply because I firmly believe that scientific research should be reproducible. And a little because I‚Äôm a nerd who enjoys figuring this kind of thing out.\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "teaching/intro-to-rmarkdown/index.html",
    "href": "teaching/intro-to-rmarkdown/index.html",
    "title": "Introduction to R Markdown",
    "section": "",
    "text": "This workshop was part of the Researcher Summer school at Keele University 2022.\nThe session and accompanying website aims to provide a gentle introduction to using R Markdown, with a focus on scientific writing.\nIt also touches on the differences between R Markdown and the next-generation Quarto.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to R\n\n\n\nLecture series\n\nAcademic\n\nReproducible Research\n\nBioinformatics\n\n\n\n\n\n\nOct 22, 2024\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to R Markdown\n\n\n\nWorkshop\n\nAcademic\n\nReproducible Research\n\n\n\n\n\n\nJun 15, 2021\n\n\nMateus Harrington\n\n\n\n\n\nNo matching items\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2021-05-10-the-sovereign-of-science/index.html",
    "href": "blog/2021-05-10-the-sovereign-of-science/index.html",
    "title": "The Sovereign of Science",
    "section": "",
    "text": "The earliest lesson I remember learning about science as a child, was that ‚Äúif it‚Äôs not repeatable, it‚Äôs not science.‚Äù Fast forward many years, I find myself as a PhD student completely baffled as how there appears to be no real incentive for scientists to do their job well Instead, scientists are subject to a ‚Äúpublish or perish‚Äù system which compels scientists to purse publications at any cost.\nSome have talked about a ‚Äúreproducibly crisis‚Äù, which often refers to the softer sciences such as psychology, but it‚Äôs just a prevalent in the life sciences, with many scientists, myself included, suspecting that over half of scientific literature is wrong.[1‚Äì3] From my perspective it seems stranger to expect quality science when scientists are both inadequately trained, given no reason to put the effort in the do better, and in many cases, actively incentivised to do science poorly.\nThere have been some token efforts to push for change, such as encouraging open access publishing and publication of experimental data with papers. These are certainly steps in the right direction, but I don‚Äôt feel they address the underlying issues.\nFirstly we should recap how science currently works.\n\nThe current scientific process\nThe vast majority of funding for science comes from the public purse via various funding bodies, and from charities like the British Heart Foundation. They will put out calls for grants, which scientists will then apply for by proposing a piece of research they will do if awarded the funding. Remember this part for later. Assuming the scientist is awarded a grant they will then use the money to conduct their experiments, and write up their work to be published in a scientific journal. We publish science in order to disseminate our work, so that other scientists can learn from and build on it.\nAt this stage scientists will waste a large amount of time tailoring their writing to the asinine formatting rules of the particular journal (things like how images should be handled or references should be structured). They then pay a small fee (typically ~¬£20-70) for the editor to look at it and decide if the paper is a good fit for the journal (there are thousands of journal and they all seek to cover different subject matters). Assuming the editor approvers, they will then request other scientists to review the paper to try and ensure the quality of the work, a process called ‚Äúpeer review‚Äù. Importantly, the scientists who do the peer review are volunteers and not paid for their review.\nIf the peer reviews have any issues or concerns these are anonymously relayed to the paper authors who are given a chance provide a rebuttal. This typically involves makes some changes to the paper to address the reviewers concerns, and this process can go through multiple rounds. If the reviews are satisfied the paper is good enough, they‚Äôll approve it for publication.\nAt this point, if the authors do nothing, their paper will be published and locked behind a pay-wall, such that anyone who actually wants the read it will be to either pay a fee to have temporary access to that specific paper (costs range wildly from tens to hundreds of pounds), or paying for a subscription to that journal to access all their papers. Alternatively the authors can pay a fee for their paper to be ‚Äúopen access‚Äù where anyone can read it for free. This fee varies from ~¬£1,000 to upwards of ¬£4,000 per paper depending on the journal.\nNature, one of the biggest journals in science, announced they will charge up to ‚Ç¨9,500 ($11,500) per paper last year. Others have pointed out that this is more than many scientists get paid in a year. Please take a moment to applaud Nature for being so boldly exploitative and reminding us that academic journals are just for-profit companies that will do anything they can get away with.\nSo to summarise:\n\nFunding body (normally public or charity funds) awards grant to scientist/s\nScientist/s conduct work and write it up to publish in a journal\nThe work is review by other scientists, and ultimately published\n\nNow if you‚Äôre paying attention you may be wondering:\n\nWhat value have the journals added to this process?\n\nThe journal certainly wastes a lot of time for the authors, and in return they either lock the science behind a pay-wall where it can only be read by scientists if their institution pays thousands to millions for journal subscriptions, or the funding body, who already paid for the science to be done, has to pay a massive fee for people to actually be able to disseminate the research.\n\n\nThe role of the journals\n\n\n\n\n\n\nFigure¬†1: Image credit: Unsplash\n\n\n\nSo you may be thinking this seems completely nonsensical. We pay for science and then we pay again to read/publish. Well actually, it‚Äôs even more stupid than that. For you see, the journals have a strong bias against negative results.\nSo suppose you‚Äôre a competent scientist. You design a good experiment, carry out a statistically appropriate analysis of the data, and it turns out your data doesn‚Äôt support your hypothesis. That‚Äôs great! As a fellow scientist, I want to know about negative results just as much as positive ones. This will stop me from wasting time and money pursing ideas that won‚Äôt work. Negative results can also help further our understanding of the subject just as much as positive ones. We can start investigating why it didn‚Äôt do what you/we may have initially expected. Except the journals won‚Äôt publish your work because they think negative results aren‚Äôt good clickbait.\n\n\nThe role of the funders\n\n\n\n\n\n\nFigure¬†2: Image credit: Unsplash\n\n\n\nBut you need to publish your results otherwise you won‚Äôt be able to attract further funding and advance your career. The funding bodies don‚Äôt care that you‚Äôve done a good piece of scientific research, they‚Äôre much too lazy to actually read the work you do. They‚Äôd rather just look at how many publications and citations you have. That‚Äôs much quicker after all, and you don‚Äôt need any scientific or statistical knowledge to understand it either. So you have no choice but to start massaging your data to try and find a positive result the journal will accept. A common form of this is often referred to as P-Hacking.[4]\n\n\nThe role of universities\nSo where do the research institutes fit in? Their sole motivation is to attract funding because they get a cut of the dosh, and it makes them look good. They therefore also don‚Äôt care about the quality of the science, and don‚Äôt typically bother teaching their graduates the skills they need to do science well. Most importantly statistics, but also methodologies for reproducible research are either not taught at all, or taught in a single extremely token module that students will have completely forgotten by the time they graduate. But they‚Äôll still be able to attract funding for research as a scientist so why would the university care?\nThere is simply no connection between the quality of a scientists work, and their success as a scientist. If you understand the incentive structure scientists are subject to, it‚Äôs obvious that science would end up being low quality and unreproducible.\n\n\nA sad example\nBut maybe you‚Äôre a real optimistic soul, and you think I‚Äôm being too cynical. Allow me to describe the most depressing moment of my PhD thus far. I‚Äôll be vague with some specifics to avoid anyone getting into trouble.\nSome time ago, I was attending a spinal cord injury (SCI) conference. This was prior to COVID-19. The last session of the last day was an open debate. The leading experts in the field begun debating that state of the field. They argued we had be doing the same thing for decades, mostly finding a million ways to grow neurons in a dish, very successfully I might add, but that this had not, and would never likely translate into anything resembling a therapy to treat SCI. Therefore, we needed to go back to basics, to better understand the pathophysiology of SCI, if we were even to make real progress.\nIt wasn‚Äôt really a debate as they pretty much all agreed with each other. I certainly agreed, and still do. At this point they started talking about specific approaches we could take as a field to achieve this.\n\nFor me, this debate was the purest, most beautiful expression of science I had even seen. It was exciting. But then someone stood up and took the mic.\n\nFrom their voice you tell they were annoyed. This individual effectively expressed that these experts at the top of their field were being too negative. That we had heard about all this ‚Äúamazing‚Äù work during the conference, and that everything was fine as is, in spite of the total lack of any meaningful advances in the field for past several decades. And so the room was silenced, and the debate ended.\nMaybe you‚Äôre wondering why the experts didn‚Äôt try to reason with this individual. The person obviously wasn‚Äôt a scientist, they didn‚Äôt offer any specific technical criticism. They just didn‚Äôt like the implication that the research done so far hadn‚Äôt produced much of worth. Surely, all they needed to do was explain that our wish as scientists was to make meaningful progress, with the ultimate goal of helping patients as soon as possible?\nBut there was a good reason they couldn‚Äôt talk back to this person. This person was on the board for one of the main funding bodies in the field. This was the most powerful person in the room. This was the hand that feeds, and none of them would dare bite it for fear they‚Äôd never get another grant.\n\nThis was our Sovereign.\n\nThey had borne witness to the most wonderful expression of science I had ever seen and they hated it.\n\nIn this moment I realised something. We weren‚Äôt scientists. We weren‚Äôt here to expand the boundaries of human knowledge, to solve problems or to cure disease. We were clowns, here merely dance so that this idiot child could tell themselves that they were making a difference.\n\nThe funding bodies are the most powerful players in science. If they say jump, we‚Äôll jump. Otherwise we lose our job.\nSuggesting that scientists are to blame for this ridiculous system is nothing more than scapegoating. This is very similar to how manufactures have successfully shifted the burden of waste management onto the public, and distracted them by pushing the lie of recycling.\nThe ‚Äúreproducibility crisis‚Äù is one of their making, and they should be the ones to solve it. Or at the very least, stop making it worse.\n\n\nThe personal cost of incompetent management\nThis problem has been on my mind a lot lately. As a third year PhD student I‚Äôve been thinking about what I‚Äôm going to do once I‚Äôve submitted my thesis.\nI was in a very fortunate position as my supervisor had managed to secure some funding for me to carry on my research. My current lab is small, but the people are both lovely on a personal level, and extremely supportive professionally as well. Sadly, this isn‚Äôt always a given. Having worked in several labs I‚Äôve seen some pretty callous and petty behaviour in other places. So this offer was plenty tempting in a lot of ways.\nBut because of my burning contempt for the way science is done, I wasn‚Äôt feeling particularly keen to stay in academia. What was the point? I have no interest in being a clown for these wretched, incompetent funding bodies for the rest of my life. To constantly have to fight to justify my existence whilst they effectively demand I cast aside my integrity to appease them and progress in my career.\nAnd yet, I got this far because I firmly believe that science is one of the most important human endeavours. I‚Äôm also pretty sure I‚Äôm better at it than most scientists, though that‚Äôs less me thinking I‚Äôm great, and more that the bar is so depressingly low.\nThrough all these thoughts, I was headhunted for a data science role as well. The job seemed likely to be rather boring, but it offered better job security and probably more money in the long term, and freedom from ‚Äúpublish or perish‚Äù. The absurdity of current science made it seem pretty attractive to me. I wonder how much great young talent is lost from science due to these things.\n\nThat may well be the greatest cost of all this stupidity.\n\nAnd then I was offered a bioinformatics postdoc in the field of dementia at a Russle Group university. After much internal strife I‚Äôve decided I‚Äôll give academia another chance. Maybe I‚Äôm just worn down by this pandemic, and the stress of my looming submission deadline. Maybe a bigger lab and a field with more funding will improve my perception. Perhaps the larger sums attract more competition and so standards of the funders are higher.\nI certainly hope so.\n\n\nSo what can be done?\nWell, writing this has made me sad, so I‚Äôm going to have a break now. Once I‚Äôve recharged, I‚Äôll aim to write a follow-up article exploring ideas for how science could be improved.\nUpdate:\nThat article has been written and can be found here\n\n\n\n\n\n\n\n\n\n\n Back to topReferences\n\n[1] Smaldino PE, McElreath R. The natural selection of bad science 2016;3:160384. https://doi.org/10.1098/rsos.160384.\n\n\n[2] Wen H, Wang H-Y, He X, Wu C-I. On the low reproducibility of cancer studies 2018;5:619‚Äì24. https://doi.org/10.1093/nsr/nwy021.\n\n\n[3] Baker M. 1,500 scientists lift the lid on reproducibility 2016;533:452‚Äì4. https://doi.org/10.1038/533452a.\n\n\n[4] Head ML, Holman L, Lanfear R, Kahn AT, Jennions MD. The extent and consequences of p-hacking in science 2015;13:e1002106. https://doi.org/10.1371/journal.pbio.1002106.\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2022-10-21-testing-normality-in-r/index.html",
    "href": "blog/2022-10-21-testing-normality-in-r/index.html",
    "title": "Testing Normality in R",
    "section": "",
    "text": "Many a statistical test assume the data follows a normal (AKA Gaussian) distribution. These tests are called parametric tests as their validity depends on the distribution of the data.\nIt is therefore important to access the normality of data if we wish to use parametric tests.\nNote that the central limit theorem tells us that with a large enough sample size (~&gt;40), data will trend to be normally distributed, so one can often get away with using parametric tests in these cases.\n\nSetup\nLet‚Äôs start by loading our packages. We‚Äôll use ggpubr as it makes nice plots, and has a handy function for Q-Q plots, and cowplot for arranging plots into grids.\nrstatix is for pipe-friendly versions of the base R stat functions, and tidyverse for data manipulation.\nNote that librarian is a neat package for automatically installing and loading packages!\n\n\nToggle the code\nlibrary(librarian)\npkg &lt;- c(\"ggpubr\", \"cowplot\", \"tidyverse\", \"rstatix\")\nshelf(pkg)\nrm(pkg)\n\n\n\n\nVisualising the data\nOne the first and best places to start when assessing normality is to simply plot the data!\nWe can use the base R rnorm function to generate a random set of normal distributed values for demonstration:\n\n\nToggle the code\n## set a seed so our \"random\" numbers don't change\nset.seed(123)\n## generate a random normal distribution\nnormal_distribution &lt;- rnorm(1000)\n## make a density plot\np1 &lt;- ggdensity(\n  normal_distribution,\n  fill = \"lightgray\",\n  title = \"Normally distributed data\"\n)\n## make a Q-Q plot\np2 &lt;- ggqqplot(normal_distribution, title = \"Normally distributed data\")\n## arrange plots into a grid\nplot_grid(p1, p2)\n\n\n\n\n\n\n\n\n\nNotice the classic bell-shaped curve in the density plot and the nice straight line in the Q-Q plot? These are the hallmarks of a normally distributed dataset.\nNow let‚Äôs see what a skewed distribution looks like:\n\n\nToggle the code\nskewed_data &lt;- rbeta(1000, 10, 2)\np1 &lt;- ggdensity(skewed_data, fill = \"lightgray\", title = \"Skewed data\")\np2 &lt;- ggqqplot(skewed_data, title = \"Skewed data\")\nplot_grid(p1, p2)\n\n\n\n\n\n\n\n\n\nSee how the density plot ‚Äúleans over‚Äù to the right and the points in the Q-Q plot curve away from our theoretical gray lines?\nQ-Q plots (or quantile-quantile plots) draw the correlation between a given sample and the normal distribution. A 45-degree reference line is also plotted. In a QQ plot, each observation is plotted as a single dot. If the data are normal, the dots should form a straight line.\n\n\nShapiro-Wilk‚Äôs normality test\nIn cases of strong skew, like the data above, visual inspection if fine, but in cases of weaker skew it can be more ambiguous.\nSo another method is to use a significance test comparing the sample distribution to a normal one in order to ascertain any deviation from normality.\nThere are a few methods to evaluate normality, including the Kolmogorov-Smirnov (K-S) normality test and the Shapiro-Wilk‚Äôs test.\nShapiro-Wilk‚Äôs method is better powered than K-S and so tends to be the recommend approach.\nLike most common statistical tests, R has handy included function for computing them:\n\n\nToggle the code\nshapiro.test(normal_distribution)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  normal_distribution\nW = 0.9983761923, p-value = 0.476468592\n\n\nToggle the code\nshapiro.test(skewed_data)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  skewed_data\nW = 0.9487249143, p-value &lt; 2.220446e-16\n\n\nAs we can see, the normally distributed data has a high p-value, tell us it is likely to be normally distributed (shocking I know!). By contrast, the skewed data has a very low p-value, indicating it is unlikely to be normally distributed (also shocking!).\n\n\nComparing groups\nIf we have two independent groups, we‚Äôd want to check their normality separately. If the underlying distributions are truly different, then they wouldn‚Äôt be normally distributed if measured together (it‚Äôd be bimodal in the case of 2 groups).\nThis is nice and easy to do with long-format data. We‚Äôll use the built-in ToothGrowth dataset to demonstrate:\n\n\nToggle the code\n## display some random rows of ToothGrowth\nToothGrowth %&gt;%\n  sample_n_by(supp, dose, size = 1)\n\n\n# A tibble: 6 √ó 3\n    len supp   dose\n  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n1  21.5 OJ      0.5\n2  23.6 OJ      1  \n3  26.4 OJ      2  \n4   5.2 VC      0.5\n5  16.5 VC      1  \n6  23.6 VC      2  \n\n\nToggle the code\n## check normality for long-format grouped data\np1 &lt;- ggdensity(ToothGrowth, x = \"len\", color = \"supp\")\np2 &lt;- ggqqplot(ToothGrowth, x = \"len\", color = \"supp\")\nplot_grid(p1, p2)\n\n\n\n\n\n\n\n\n\nToggle the code\nToothGrowth %&gt;%\n  group_by(supp) %&gt;%\n  shapiro_test(len)\n\n\n# A tibble: 2 √ó 4\n  supp  variable statistic      p\n  &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 OJ    len          0.918 0.0236\n2 VC    len          0.966 0.428 \n\n\nTo apply Shapiro-Wilk‚Äôs across columns is easy enough. We‚Äôll use another built-in dataset, iris, to demonstrate with wide-format data:\n\n\nToggle the code\n## display some rows of iris\niris %&gt;%\n  sample_n_by(Species, size = 1)\n\n\n# A tibble: 3 √ó 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     \n1          4.7         3.2          1.6         0.2 setosa    \n2          5.5         2.3          4           1.3 versicolor\n3          7.2         3.6          6.1         2.5 virginica \n\n\nToggle the code\n## with column names\niris %&gt;%\n  group_by(Species) %&gt;%\n  shapiro_test(Sepal.Length, Petal.Length)\n\n\n# A tibble: 6 √ó 4\n  Species    variable     statistic      p\n  &lt;fct&gt;      &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1 setosa     Petal.Length     0.955 0.0548\n2 setosa     Sepal.Length     0.978 0.460 \n3 versicolor Petal.Length     0.966 0.158 \n4 versicolor Sepal.Length     0.978 0.465 \n5 virginica  Petal.Length     0.962 0.110 \n6 virginica  Sepal.Length     0.971 0.258 \n\n\nBut for the plots, it‚Äôs a lot easier to pivot to long format:\n\n\nToggle the code\niris_long &lt;- iris %&gt;%\n  pivot_longer(Sepal.Length:Petal.Width, names_to = \"variable\")\n\nggdensity(iris_long, x = \"value\", color = \"variable\", facet.by = \"Species\")\n\n\n\n\n\n\n\n\n\nToggle the code\nggqqplot(iris_long, x = \"value\", color = \"variable\", facet.by = \"Species\")\n\n\n\n\n\n\n\n\n\nI‚Äôd recommend checking out the principals of tidy data if you‚Äôre unsure of how best to structure your own data.\n\n\nSummary\nHopefully this gives you a sense of how to check for normality. Note, that even in cases where it is fairly obvious the data is skewed, it can be worth doing a test like Shapiro-Wilk‚Äôs, if only to provide a formal justification for the non-parametric tests that you subsequently use.\nAlso note that the Shapiro-Wilk test become sensitive to small deviations from normality at larger sample sizes (&gt;50), so it‚Äôs generally good to check the visualisations and the test.\nFinally, if you have a larger sample size and still in doubt about the normality of your data, you can just use non-parametric stats to be safe. You‚Äôll lose a little power, but so long as the sample size is large enough, it‚Äôs unlikely to be critical.\n\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2022-01-26-progress-in-open-research/index.html",
    "href": "blog/2022-01-26-progress-in-open-research/index.html",
    "title": "Progress in open research!",
    "section": "",
    "text": "It‚Äôs been a while since my last post where I outlined ideas for how science may be done better. In the interim I‚Äôve submitting my PhD thesis and am impatiently awaiting my viva (where one defends their thesis to experts, which you need to pass to be awarded your PhD!). I also started my new role as a Research Associate (the posh formal title for a Post-Doc) at the Dementia Research Institute at Cardiff University which has been a good mix of exciting and terrifying.\nBut, I recently came across a really cool initiative that attempts to implement some of the ideas I explored in the last post, called the Peer Community in.\n\n\n\n\n\n\nFigure¬†1: Image credit: PCI\n\n\n\nLooking at the above outline, the PCI works by inviting authors to first submit their papers to a preprint servers with their code and data. Then the authors can submit their DOI to the PCI whereupon a member of the community can recommend the paper as worthy of review. Then others in the community are invited to review the paper, and go through a typical peer-review process with new versions being updated on the preprint server. Once the reviewers are happy that the paper is of sufficient quality they give the article a recommendation, which itself is a separate, citable document with a DOI.\nAt this stage the authors can submit their article to a PCI-friendly journal which will typically accept the recommended paper without applying their own peer-review process. The key benefit here is that the peer-review is divorced from the various biases a journal can bring to this process. The PCI also have their own Peer Community Journal which will always accept the recommendation and publish the article as open-access for free!\n\n\n\n\n\n\nFigure¬†2: Image credit: PCI\n\n\n\nThis last option of publishing with the Peer Community Journal is particularly appealing to me, especially at a time where other journals are exploiting their position to charge an outrageous open-access fee of up to ‚Ç¨9,500 ($11,500) per paper. I won‚Äôt rehash my rant outlining my feeling towards these parasitic journals, if you need a refresher though, that post can be found here.\nThey also have a video explaining the process I‚Äôd recommend watching:\n\nI will certainly support this model in my future work as much as I am able to, and encourage any of my peers to do the same. I truly hope this model finds success as I firmly believe it will lead to fairer and better quality science.\nIf you‚Äôre keen to support PCI, please see their guidance here!\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2025-06-01-first-soldering-adventure/index.html",
    "href": "blog/2025-06-01-first-soldering-adventure/index.html",
    "title": "Keyboard soldering adventures",
    "section": "",
    "text": "So, I‚Äôve gone down a bit of keyboard rabbit hole lately!\nIt started with me getting an unusual ergonomic keyboard in the form of a Kinesis Advantage 2 after a friend recommended it. This is a split keyboard with two concave keywells, but a picture will make more sense I suspect:\n\nI won‚Äôt attempt to review this keyboard, but sufficed to say I liked it after a period of adjustment, and was happily using it for several months. The issue arose when I started going into the office more often and became annoyed with the plebeian normal keyboard at work (BORING!).\nI considered just getting another one, or maybe the newer wireless version from the same manufacturer, but they‚Äôre a little pricey and I had my curiosity peeked by the Corne keyboard. The Corne is a columnar stagger 40% keyboard, with a PCB designed by the amazing Foostan and it has become one of the most popular keyboards among the split community.\n\nIt‚Äôs even popular enough that there are some vendors that will sell you a pre-assembled Corne, but half the fun is being able to customise things to your fancy! So I opted to get a kit that I could assemble myself, but I don‚Äôt own any soldering equipment (more on that later), so I went for the pre-soldered board which effectively leaves you with a soldered PCB, the switches, keycaps and case of your choice to put together.\nThe planets must have been aligned in an unusual fashion because I actually had the wherewithal to take photos as I put it together:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPutting those switches in was most the stressful part as that has fears of breaking the plate if too forceful‚Ä¶ I went with Kailh Choc Pro Reds for the curious, but I‚Äôm not switch connoisseur, so very open to other suggestions if you reckon something else‚Äôd be better!\nI somehow succeeded without breaking anything (move evidence of strange planetary movementsüåö) and the final result was pretty sweet I reckons!\n\nAfter much time faffing with key mappings and a bit more practise with the new layout, I‚Äôve ended up with a setup I‚Äôm really enjoying!\n\nYou can find my keymap config file here if curious. One aspect of the layout I‚Äôve particularly liked was using homerow mods and having an easy hyper key (‚åÉ‚å•‚áß‚åò).\nFor the uninitiated, the firmware you can flash on these keyboards is very versatile. For example, it lets you have keys that can have one output if you tap the key, and another if you hold the key. Homerow mods take advantage of this to let you use your home row keys as modifiers (‚åÉ, ‚å•, ‚áß, ‚åò) when held.\n\nThe hyper key is just a very awkward series of keys to hold on a normal keyboard, and so no applications will use them as keybindings. This opens up a whole world of possibilities to use in conjunction with hyper. In my case I‚Äôm using them for window management on the Mac I use for work (I miss my Arch Linux machine from the PhD days üò≠). It‚Äôs certainly not the same as having a proper window manager, but it gets close enough (and yes I‚Äôve tried Yabai, I found it finicky).\nBut I digress, the issue this lovely keyboard caused was frustration with my Kinesis. You see, whilst the hardware of the Kinesis is plenty nice, the firmware is infuriatingly limited.\nTo give just one example, one can only have two layers of keybindings on the Kinesis (don‚Äôt ask me why‚Ä¶). And homerow mods? Forget about it!\nAnd yes, I didn‚Äôt forget that I got the Corne in the first place was because I liked the Kinesis‚Ä¶ The universe has an annoying sense of humour, and so further down the rabbit hole we go\n\nSomewhat reassuringly it turns out I was not the only one annoyed with the limitations of the Kinesis firmware, and the lovely Michael Stapelberg made a super cool mod for the keyboard to solve it! This mod involves swapping out the controller for the keyboard for one we can use with other, better (and open-source) firmware, such as QMK. Micheal designed the PCB for this and very helpfully lays out all the components you‚Äôll need for this in a wonderfully well-documented repo here, but importantly, you need to solder it all together yourself!\n\nAs a side note, it turns out some PCB manufacturers will actually do all this for you (for extra dosh of course), but I did not know this at the time because my ignorance is vast. Even if I had known I think I would‚Äôve gone down the DIY route anyway, but still, the more you know I guess.\nNow I‚Äôve never soldered before, but it seemed to be nice and beginner friendly project so I was down to try! The issue being that I own no soldering equipment, and don‚Äôt really have physical space in my flat to put any (and don‚Äôt fancy spending loads of money building a whole setup!)üòë\nBut then it occurred to be that there must be some sort of community space for this kind of equipment somewhere in Cardiff, and it turned out there is! Pretty much the first hit on searching was the super cool RemakerSpace, a not-for-profit that offers fancy 3D printing, industrial sewing machines and a well equipped soldering workshop. A few emails later and I got the okay to rock up and give it a bashüòé\n\nThe place is super well equipped, and I‚Äôm now presumably totally spoiled and will never be able to use less fancy equipmentü§∑\nThe planets were sadly not on side this time and so I wasn‚Äôt diligent about taking pictures whilst doing this, but here‚Äôs the unadorned PCB:\n\n\n\n\n\n\n\n\n\n\nAnd here‚Äôs the finished PCB in all it‚Äôs glory:\n\n\n\n\n\n\n\n\n\n\nI didn‚Äôt burn myself, set anything on fire or die and I even had fun, so I regard it as a great success!\nI also had no way of knowing if I screwed anything up at the time since I didn‚Äôt realise the controller board had a wretched micro-USB port, which I (obviously) didn‚Äôt think to bring, and so chose to roll with unjustified confidence through it assuming it would work. Soldering the LEDs and resistors on the back was definitely the hardest bit as it‚Äôs the most fiddly. I‚Äôm sure there were tools in there that could have helped (probably those funky helping hands on the multimeter in the pic of the setup above), but I was alone and was too shy to mess around with stuff too muchüòÖ\nNow it was just a case of removing the rubbish original board:\n\nAnd installing the shiny new one without breaking the scary ribbon cables:\n\nAnd after some tense firmware flashing it worked!!!\n\nTo perhaps little surprise it seems the LEDs don‚Äôt actually work (maybe I installed them wrong way aroundüòÖ), but who needs that junk anyway! The only other issue I‚Äôve noticed is that it takes a strangely long time to start working on my Mac from sleep, but it‚Äôs only a few seconds so I don‚Äôt much care and have been too lazy to investigate for now.\nFor the curious here‚Äôs my current keymap on the QMK configurator:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI had loads of fun doing this and would highly recommend as a first soldering project to the 3 people this might be relevant for :P\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2021-05-11-how-might-science-be-done-better/index.html",
    "href": "blog/2021-05-11-how-might-science-be-done-better/index.html",
    "title": "How might science be done better?",
    "section": "",
    "text": "So in my previous article on The Sovereign of Science, I ranted about how I feel the reproducibility issues science face are a result of the lack of feedback scientists receive pertaining to the quality of their work. Or, to put it more simply, none of the masters of science (the funding bodies, journals and universities) care if their science is any good, and so it inevitably isn‚Äôt.\nHopefully we can agree that this isn‚Äôt ideal.\n\nBut what can we do about?\n\nWell I can‚Äôt do much as a lowly PhD student. I simply don‚Äôt have the power. However, if you happen to be head of a major funding body, and have presumably been struck by some strange cosmic radiation that has inspired you to suddenly realise you should probably care about the quality of the science you fund, then I have ideas.\nI should stress that I don‚Äôt think these ideas are perfect. But I do feel they outline an objectively better way of doing science, that saves scientists time, saves money, and finally rewards scientists for doing quality research, thus improving the quality of science as a whole and eventually addressing the reproducibility problem.\n\nFirstly: The journals gotta go\n\n\n\n\n\n\nFigure¬†1: And never come back! Image credit: Unsplash\n\n\n\nSeriously. The only contributions to science they make are harmful ones:\n\nThey reduce the accessibility of science by locking it behind paywalls\nThey don‚Äôt care about quality\nThey steal billions from both funders and universities\n\nSo why put up with them?\n\nI propose that the major research councils either, take 1% of what they spend a year on open access fees, and instead create a new platform where the research they fund will be published, or just throw their weight behind the Peer Community in.\n\nThe latter makes more sense to me as it‚Äôd require less effort on their part, but I propose the former as I suspect the people who run these bodies may prefer to create their own initiative for egotistical reasons. Not that I‚Äôm cynical‚Ä¶\n\n\nA new publication model\n\n\n\n\n\n\nFigure¬†2: Image credit: Unsplash\n\n\n\nAll the work published here will of course be freely available. In order to publish, authors would be required to create an account, with their real name, contact information and crucially, their research institution.\n\nThis is so the platform has no anonymity and both individual scientists, and their institutions can be held accountable for any poor behaviour.\n\nAs soon as a paper is uploaded by the authors, it would only be viewable by peer reviewers. Before the paper is freely viewable, the authors would be required to review as many papers as their own has authors. So if the paper has 5 authors, they must peer review 5 papers. This could all be done by one author, each author could do 1, or anything in between.\n\nThis ensures that there will always be enough peer reviewers available.\n\nOnce they have completed this requirement, the paper would go live. If the paper has received at least 2 peer reviews supporting the quality of the work, it would be designated as having been reviewed and approved. If not, it would instead be designated as ‚Äúunder review‚Äù.\n\n\nRethinking peer review\n Here is what I suspect will be my most controversial suggestion:\n\nThe peer review process should be completely transparent.\n\nI haven‚Äôt really addressed this till now, but you may have thought something along the lines of:\n‚ÄúIsn‚Äôt the peer review process supposed to ensure only good science gets published?‚Äù\nAnd yeah, that sure is what it‚Äôs supposed to do. Unfortunately, the state of science clearly shows it isn‚Äôt working well enough. The peer review process is anonymous, so authors never know the identity of their reviewers, and the comments they make are also never made public.\nThe argument in favour of this system is that it allows the reviews to speak freely, without fear that the authors may retaliate by refusing collaborations, or returning harsher reviews of their own work. In smaller fields the authors and reviewers may well know one another personally, and so a transparent system could also be a source of conflict.\nI disagree with this assessment. If peer review were completely public, and either a reviewer or author/s behaved poorly, they could warned, denied future funding or even removed by their own institution, as this behaviour would reflect poorly on them as well. I suspect this would be enough to keep peer review civil.\nIt would also allow us to hold peer reviewers to account if they give bad science a pass. But why would they ever do that? Well, as I touched on in the prior post, scientists are often woefully statistically incompetent, so even if they read the paper properly, they may be too ignorant to notice poor practices such as a study being underpowered, or data not meeting the assumptions of the statical tests performed.\nAnd remember, peer reviewers are volunteers, they don‚Äôt get paid or rewarded at all for their time under the current system.\nSo imagine: you are invited to review a paper in your field. You accept, thinking you aren‚Äôt too busy at the minute and you‚Äôll be able to squeeze it in. But then you remember the abstract deadline for a conference is fast approaching. You also need to finish writing some grants. And you have your own papers to write, teaching to do, and stupid university bureaucracy to waste your time on. So you forget about the paper you said you‚Äôd review until the editor says the deadlines approaching soon. Now you‚Äôre really scrambling. You quickly skim through the paper, hope the other reviewer reads it properly and give the authors the benefit of the doubt that it‚Äôs probably fine.\nAgain, there‚Äôs no reward for doing this at all, let alone for doing it well, and no punishment if you do it poorly. So you cut corners. And this is how at lot of science goes.\nThe opposite can also happen, where a reviewer just doesn‚Äôt like your results and gives you a hard time, but as an author you need their approval to publish, so you end up doing whatever they want, even if you don‚Äôt really agree with them.\nBy making the whole process open, it forces both authors and peer reviewers to take it seriously and do it well. And even once a paper has been reviewed twice, it could still be reviewed by other academics if they feel they‚Äôve something to add. This way, even if both the first reviewers miss something, it‚Äôs very likely that someone else will eventually notice an issue and publicly notify the authors.\nA system like this also lends itself well to making small tweaks. Perhaps early career researchers could get something like a ‚Äúlearner‚Äù badge, where their peer reviews are only worth half as much as more experienced colleagues. PhD students could be required to do a certain number of peer reviews in order to be awarded their doctorate.\nTo add to this, I would show the latest iteration of a given paper on the website, but I would allow users to go back and see each prior version of the paper. This could be particularly useful for students to learn from.\n\n\nProviding a reason to care about reproducibility\n\n\n\n\n\n\nFigure¬†3: Image credit: Kristian Niemi\n\n\n\nI would also propose papers be given some kind of ‚Äúreproducibility score‚Äù. I imagine this functioning like a checklist:\n\nIs the raw experimental data published?\nIs the paper itself written in plain text, with reproducible figures/tables?\nIs the script/s used to analyse the data included?\nIs the software used free and open-source?\nWas version control used for the script/s and/or paper?\nIs a container with all the software used provided?\n\nIf all the criteria are met, then the paper gets a perfect score. This score could be tacked at both the level of individual scientists, and for institutions. This would serve as a nice lazy indicator as to the quality of the scientist/institution. Funding bodies could use this when awarding grants, and postdocs/students could use it to help decide where they work/study.\nAs has been suggested elsewhere, I would also provide a separate doi for the paper, data and container, so these can receive citation and the authors can continue to receive proper credit for their work.\n\nThe key here is that this provides an incentive for scientists to make their research more transparent and reproducible, not to evaluate the impact or even the quality of the work itself. The open peer review should sort that part. Hopefully‚Ä¶\n\nIf their studies are underpowered, or their stats rubbish, this system would make it obvious and easy to call out. This would reflect poorly on the authors, and perhaps more importantly on their institution. Thus, it would behove them to make sure their researchers are well trained and equipped to follow best practises.\n\n\nSounds great, but who pays for this system?\n\n\n\n\n\n\nFigure¬†4: Image credit: Unsplash\n\n\n\nI would suggest a combination of the funding bodies and major research institutions. Pragmatically, I think it makes sense for universities to provide the servers to host their own research, and for software to automatically back this up to other university servers. This way, even if the servers are damaged/destroyed (in a fire for example), nothing would be lost.\nThe cost of this would be extremely low, especially compared to what is currently spent by funders on open access fees, and most definitely less than the literal billions universities are spending on journal subscriptions. I‚Äôm pretty confident this proposed system would cost less than 1% of what is currently spent to both establish and maintain. And it would mean scientists don‚Äôt have to waste huge amounts of time bending over backwards to appease asinine journal requirements.\nIt‚Äôs not like putting a .pdf on the internet costs a lot, which effectively all publication entails. This website costs me exactly nothing. The system I‚Äôm proposing is effectively just a big distributed website with a particular structure, so the cost would be very low, especially when distributed across all research institutions.\n\n\nFeedback welcome\n\n\n\n\n\n\nFigure¬†5: Sometimes I wonder if I‚Äôm just too idealistic. Image credit: Unsplash\n\n\n\nThis is of course just my 2 cents. And I should reiterate, I‚Äôm sure these proposals would have their own issues, and certainly introduce some major teething pains. But I still feel it‚Äôs a big improvement over what we‚Äôve got. If you‚Äôve any feedback, positive or negative, it‚Äôs very welcome. If it‚Äôs really good I may even update these articles.\nPerhaps I‚Äôm getting ahead of myself though. I didn‚Äôt write these expecting to start a revolution. We have the system we do because the people with power are either ignorant, apathetic or both, and I don‚Äôt expect that to change anytime soon.\nThis was more an exercise in articulating my thoughts, and getting some of this vitriol out of my head. And I‚Äôve enjoyed it, so I‚Äôll chalk it up as a win.\nUpdate: So embarrassingly enough, when I first wrote this I wasn‚Äôt aware of the Peer Community in, which actually makes a lot of good progress in respect to the peer-review side of things, particularly in trying to de-couple peer-review from journals, though I still think some of the other ideas I outline here could improve on their model. Having said that, it‚Äôs a really cool initiative, and I hope to publish my future work with them where possible! I write more about them in this post.\n\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Gabriel Mateus Bernardo Harrington",
    "section": "",
    "text": "Hi!\n\nThanks for stopping by!\nI am a postdoctoral bioinformatician at the UK Dementia Research Institute, Cardiff University, I specialise in Alzheimer‚Äôs disease with a vascular lens. Learn more about my research interests in publications. I‚Äôm highly experienced in single-cell sequencing analysis and am passionate about advancing reproducible research practices.\nI currently chair the Early Career Researcher (ECR) Informatics Committee and co-founded the UK DRI‚Äôs Reproducibility Working Group following significant interest in my presentations on the topic."
  },
  {
    "objectID": "about/index.html#about-me",
    "href": "about/index.html#about-me",
    "title": "Gabriel Mateus Bernardo Harrington",
    "section": "About me",
    "text": "About me\n\n\nBiologist turned informatician, passionate about improving research standards.\nI feel in love with R when I first learnt to code during my PhD, and rapidly learned there were objectively better ways of working that many researchers were oblivious too. This ignited a drive to fight to improve research practices, including educating colleagues and students. I‚Äôve since gotten more involved in teaching, recieving the Higher Education Associate Fellowship and teaching an introduction to R course for our bioinformatics Masters students at Cardiff University.\n\n\n\n\n\n\nPhD in Biomedical Engineering ‚àô Cardiff University ‚àô 2021\n\n\nBSc in Biological Sciences ‚àô Lancaster University ‚àô 2016\n\n\n\n\nI completed my PhD in biomedical engineering at Keele Univeristy in 2021, where my research focused on biomarkers and prognosis of spinal cord injury. I gained lots of experience working with electronic healthcare records and my first taste of omics in the form of proteomics.\nI got my BSc in biological sciences from Lancaster University in 2016, followed by an internship at the prestigious Bionics institute in Melbourne, Australia. There, I contributed to groundbreaking research in viral gene therapy and optogenetics in the cochlea, working alongside an internationally renowned research team. This experience involved comprehensive laboratory work including surgeries, tissue processing, immunohistochemistry, and microscopy, contributing to the development of next-generation cochlear implants.\nYou can find my CV here"
  },
  {
    "objectID": "about/index.html#lately",
    "href": "about/index.html#lately",
    "title": "Gabriel Mateus Bernardo Harrington",
    "section": "Lately ‚Ä¶",
    "text": "Lately ‚Ä¶\n\n\nTeaching\n\n\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\n\n\nNo matching items\n\nSee all ‚Üí\n\n\nTalks\n\n\n\n\n\n\n\n\n\n\nProteomic and Bioinformatics Analyses of Plasma From Sci Neurological Improvers and Non Improvers\n\n\n\n\n\n\n\n\nNo matching items\n\nSee all ‚Üí\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\nKeyboard soldering adventures\n\n\n\n\n\nNo matching items\n\nSee all ‚Üí\n\n\nPublications\n\n\n\n\n\n\n\n\n\n\nA comprehensive proteomic and bioinformatics analysis of human spinal cord injury plasma identifies proteins associated with the complement cascade and liver function as potential prognostic indicators of neurological outcome\n\n\n\n\n\nNo matching items\n\nSee all ‚Üí"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Academic publications",
    "section": "",
    "text": "A comprehensive proteomic and bioinformatics analysis of human spinal cord injury plasma identifies proteins associated with the complement cascade and liver function as potential prognostic indicators of neurological outcome\n\n\n\nAcademic\n\nPaper\n\nPrognostic model\n\nProteomics\n\nSpinal Cord Injury\n\n\n\n\n\n\nOct 25, 2022\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nA multimodal approach to biomarker discovery for spinal cord injury\n\n\n\nPrognostic model\n\nProteomics\n\nAcademic\n\nPublication\n\nSpinal Cord Injury\n\nR Markdown\n\n\n\n\n\n\nJun 23, 2022\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nRoutinely measured haematological markers can help to predict AIS scores following spinal cord injury\n\n\n\nSpinal cord injury\n\nPrognostic model\n\n\n\n\n\n\nJul 27, 2020\n\n\nGabriel Mateus Bernardo Harrington, Paul Cool, Charlotte Hulme, Aheed Osman, Joy Chowdhury, Naveen Kumar, Srinivasa Budithi & Karina Wright\n\n\n\n\n\n\n\n\n\n\n\n\nA Preliminary Cohort Study Assessing Routine Blood Analyte Levels and Neurological Outcome Following Spinal Cord Injury\n\n\n\nSpinal cord injury\n\nPrognostic model\n\n\n\n\n\n\nJan 9, 2020\n\n\nGabriel Mateus Bernardo Harrington, Sharon Brown, Charlotte Hulme, Rachel Morris, Anna Bennett, Wai-Hung Tsang, Aheed Osman, Joy Chowdhury, Naveen Kumar & Karina Wright\n\n\n\n\n\nNo matching items\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "publications/2022-10-25-a-multimodal-approach-to-biomarker-discovery-for-spinal-cord-injury/index.html",
    "href": "publications/2022-10-25-a-multimodal-approach-to-biomarker-discovery-for-spinal-cord-injury/index.html",
    "title": "A multimodal approach to biomarker discovery for spinal cord injury",
    "section": "",
    "text": "This was quite the journey!\nI‚Äôm very grateful to my supervisor team for all their guidance through my PhD. I couldn‚Äôt have done it without them!\nI‚Äôm also very happy I was able to write the whole thing in R Markdown. Special mention to the bookdown and related thesisdown packages in particular. The open-source community is amazing, and I hope to give back to similar projects over the course of my career! Note that if you happen to interested in R Markdown, I‚Äôd highly recommend checking out the next generation: Quarto, it‚Äôs a wonderful refinement!\n\nAbstract\nThe extent of the devastating outcomes following spinal cord injury, particularly with respect to neurological function, are difficult to predict. This can make it difficult to plan clinical care and manage patient expectations, both of which can have profound impacts on rehabilitation and mental health. The heterogeneity in neurological recovery also make powering clinical trials challenging, stifling the development of novel therapies. Prognostic biomarkers that can aid in accurately predicting outcomes would therefore be of value in both clinical and research settings. They could also serve to uncover novel understanding as to the pathophysiological factors in heterogeneous outcomes, potentially revealing novel therapeutic targets. Whilst cerebral spinal fluid is an obvious target for identifying such biomarkers, the risk and cost associated with collecting such samples possess a challenge. Blood by contrast is easily accessible, with little risk and cost to healthcare providers, making it an ideal source for testing. This project aims to identify novel blood biomarkers for spinal cord injury via unbiased methods. Specifically, prognostic models of discharge and 12-month post-injury neurology were developed utilising admission neurology scores, basic demographic information and routinely measured haematological markers. Here, blood markers commonly associated with liver function, including alanine transaminase and alkaline phosphatase, were found to add modest but statistically significant predictive value. This would suggest that liver health, or perhaps metabolic health more broadly, is at least partially related to variance of neurological outcomes. Further, both labelled and unlabelled shotgun proteomics of plasma from human patients revealed proteins associated with the complement cascade to be of particular importance for both severity of injury, and differential functional recovery. Many of the proteins identified were also directly linked to the liver, once again suggesting the organ may have more significance to outcomes of spinal cord injury than previously appreciated.\n\n\n\n\n Back to topReuseCC BY-SA 4.0CitationBibTeX citation:@online{harrington2022,\n  author = {Harrington, Mateus},\n  title = {A Multimodal Approach to Biomarker Discovery for Spinal Cord\n    Injury},\n  date = {2022-06-23},\n  url = {https://mateusharrington.com/publications/2022-10-25-a-multimodal-approach-to-biomarker-discovery-for-spinal-cord-injury/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHarrington, Mateus. 2022. ‚ÄúA Multimodal Approach to Biomarker\nDiscovery for Spinal Cord Injury.‚Äù June 23, 2022. https://mateusharrington.com/publications/2022-10-25-a-multimodal-approach-to-biomarker-discovery-for-spinal-cord-injury/."
  },
  {
    "objectID": "publications/perlim_modelling_paper/index.html",
    "href": "publications/perlim_modelling_paper/index.html",
    "title": "A Preliminary Cohort Study Assessing Routine Blood Analyte Levels and Neurological Outcome Following Spinal Cord Injury",
    "section": "",
    "text": "Image credit: Dolman Law Group\n\n\nThere is increasing interest in the identification of biomarkers that could predict neurological outcome following a spinal cord injury (SCI). Although initial American Spinal Injury Association (ASIA) Impairment Scale (AIS) grade is a good indicator of neurological outcome, for the patient and clinicians, an element of uncertainty remains. This preliminary study aimed to assess the additive potential of routine blood analytes following Principal Component Analysis (PCA) to develop prognostic models for neurological outcome following spinal cord injury. Routine blood and clinical data were collected from SCI patients (n=82) and PCA used to reduce the number of blood analytes into related factors. Outcome neurology was obtained from AIS scores at 3- and 12-months post-injury, with Motor (AIS and Total including all myotomes) and Sensory (AIS, Touch and Pain) being assessed individually. Multiple regression models were created for all outcome measures. Blood analytes relating to ‚Äòliver function‚Äô and ‚Äòacute inflammation and liver function‚Äô factors were found to significantly increase prediction of neurological outcome at both 3 months (Touch, Pain and AIS Sensory) and at 1 year (Pain, R^2 increased by 0.025 and Total Motor, R^2 increased by 0.016). For some models ‚Äòliver function‚Äô and ‚Äòacute inflammation and liver function‚Äô factors were both significantly predictive with the greatest combined R^2 improvement of 0.043 occurring for 3m Pain prediction. These preliminary findings support ongoing research into the use of routine blood analytes in the prediction of neurological outcome in SCI patients.\n\n\n\n Back to topReuseCC BY-SA 4.0CitationBibTeX citation:@online{mateus_bernardo_harrington,_sharon_brown,_charlotte_hulme,_rachel_morris,_anna_bennett,_wai-hung_tsang,_aheed_osman,_joy_chowdhury,_naveen_kumar_&_karina_wright2020,\n  author = {Mateus Bernardo Harrington, Sharon Brown, Charlotte Hulme,\n    Rachel Morris, Anna Bennett, Wai-Hung Tsang, Aheed Osman, Joy\n    Chowdhury, Naveen Kumar \\& Karina Wright, Gabriel},\n  title = {A {Preliminary} {Cohort} {Study} {Assessing} {Routine}\n    {Blood} {Analyte} {Levels} and {Neurological} {Outcome} {Following}\n    {Spinal} {Cord} {Injury}},\n  date = {2020-01-09},\n  url = {https://mateusharrington.com/publications/perlim_modelling_paper/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMateus Bernardo Harrington, Sharon Brown, Charlotte Hulme, Rachel\nMorris, Anna Bennett, Wai-Hung Tsang, Aheed Osman, Joy Chowdhury, Naveen\nKumar & Karina Wright, Gabriel. 2020. ‚ÄúA Preliminary Cohort\nStudy Assessing Routine Blood Analyte Levels and Neurological Outcome\nFollowing Spinal Cord Injury.‚Äù January 9, 2020. https://mateusharrington.com/publications/perlim_modelling_paper/."
  },
  {
    "objectID": "publications/modelling_paper_500/index.html",
    "href": "publications/modelling_paper_500/index.html",
    "title": "Routinely measured haematological markers can help to predict AIS scores following spinal cord injury",
    "section": "",
    "text": "Image credit: apparalyzed\n\n\nNeurological outcomes following spinal cord injury (SCI) are currently difficult to predict. Whilst the initial American Spinal Injury Association (ASIA) Impairment Scale (AIS) grade can give an estimate of outcome, the high remaining degree of uncertainty has stoked recent interest in biomarkers for SCI. This study aimed to assess the prognostic value of routinely measured blood biomarkers by developing prognostic models of AIS scores at discharge and 12-months post-injury. Routine blood and clinical data were collected from SCI patients (n=427) and blood measures that had been assessed in less than 50% of patients were excluded. Outcome neurology was obtained from AIS and Spinal cord independence measure III (SCIM-III) scores at discharge and 12-months post-injury, with motor (AIS) and sensory (AIS, touch and prick) abilities being assessed individually. Linear regression models with and without elastic net penalisation were created for all outcome measures. Blood measures associated with liver function such as alanine transaminase were found to add value to predictions of SCIM-III at discharge and 12-months post-injury. Furthermore, components of a total blood count including haemoglobin were found to add value to predictions of AIS motor and sensory scores at discharge and 12-month post-injury. These findings corroborate the results of our previous preliminary study and thus provide further evidence that routine blood measures can add prognostic value in SCI, and that markers of liver function are of particular interest.\n\n\n\n Back to topReuseCC BY-SA 4.0CitationBibTeX citation:@article{mateus_bernardo_harrington,_paul_cool,_charlotte_hulme,_aheed_osman,_joy_chowdhury,_naveen_kumar,_srinivasa_budithi_&_karina_wright2020,\n  author = {Mateus Bernardo Harrington, Paul Cool, Charlotte Hulme,\n    Aheed Osman, Joy Chowdhury, Naveen Kumar, Srinivasa Budithi \\&\n    Karina Wright, Gabriel},\n  publisher = {Mary Ann Liebert, Inc., publishers},\n  title = {Routinely {Measured} {Haematological} {Markers} {Can} {Help}\n    to {Predict} {AIS} {Scores} {Following} {Spinal} {Cord} {Injury}},\n  journal = {Journal of Neurotrauma},\n  date = {2020-07-27},\n  url = {https://doi.org/10.1089/neu.2020.7144},\n  doi = {10/gg55p5},\n  issn = {0897-7151},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMateus Bernardo Harrington, Paul Cool, Charlotte Hulme, Aheed Osman, Joy\nChowdhury, Naveen Kumar, Srinivasa Budithi & Karina Wright, Gabriel.\n2020. ‚ÄúRoutinely Measured Haematological Markers Can Help to\nPredict AIS Scores Following Spinal Cord Injury.‚Äù Journal of\nNeurotrauma, July. https://doi.org/10/gg55p5."
  },
  {
    "objectID": "publications/2022-10-25-sci-human-proteomics/index.html",
    "href": "publications/2022-10-25-sci-human-proteomics/index.html",
    "title": "A comprehensive proteomic and bioinformatics analysis of human spinal cord injury plasma identifies proteins associated with the complement cascade and liver function as potential prognostic indicators of neurological outcome",
    "section": "",
    "text": "Spinal Cord Injury (SCI) is a major cause of disability, with complications post-injury often leading to life-long health issues with need of extensive treatment. Neurological outcome post-SCI can be variable and difficult to predict, particularly in incomplete injured patients. The identification of specific SCI biomarkers in blood, may be able to improve prognostics in the field. This study has utilised proteomic and bioinformatics methodologies to investigate differentially expressed proteins in plasma samples across human SCI cohorts with the aim of identifying prognostic biomarkers and biological pathway alterations that relate to neurological outcome.\n\n\n\nBlood samples were taken, following informed consent, from ASIA impairment scale (AIS) grade C ‚ÄúImprovers‚Äù (those who experienced an AIS grade improvement) and ‚ÄúNon-Improvers‚Äù (No AIS change), and AIS grade A and D at &lt;2 weeks (‚ÄúAcute‚Äù) and approx. 3 months (‚ÄúSub-acute‚Äù) post-injury. The total protein concentration from each sample was extracted, with pooled samples being labelled and non-pooled samples treated with ProteoMiner‚Ñ¢ beads. Samples were then analysed using two 4-plex isobaric tag for relative and absolute quantification (iTRAQ) analyses and a label-free experiment for comparison, before quantifying with mass spectrometry. Data are available via ProteomeXchange with identifiers PXD035025 and PXD035072 for the iTRAQ and label-free experiments respectively.\nProteomic datasets were analysed using OpenMS (version 2.6.0). R (version 4.1.4) and in particular, the R packages MSstats (version 4.0.1) and pathview (version 1.32.0) were used for downstream analysis. Proteins of interest identified from this analysis were further validated by enzyme-linked immunosorbent assay (ELISA).\n\n\n\nThe data demonstrated proteomic differences between the cohorts, with the results from the iTRAQ approach supporting those of the label-free analysis. A total of 79 and 87 differentially abundant proteins across AIS and longitudinal groups were identified from the iTRAQ and label-free analyses, respectively. Alpha-2-macroglobulin (A2M), retinol binding protein 4 (RBP4), serum amyloid A1 (SAA1), Peroxiredoxin 2, alipoprotein A1 (ApoA1) and several immunoglobulins were identified as biologically relevant and differentially abundant, with potential as individual prognostic biomarkers of neurological outcome. Bioinformatics analyses revealed that the majority of differentially abundant proteins were components of the complement cascade and most interacted directly with the liver.\n\n\n\nMany of the proteins of interest identified using proteomics were detected only in a single group and therefore have potential as a binary (present or absent) biomarkers, RBP4 and PRX-2 in particular. Additional investigations into the chronology of these proteins, and their levels in other tissues (cerebrospinal fluid in particular) are needed to better understand the underlying pathophysiology, including any potentially modifiable targets. Pathway analysis highlighted the complement cascasde as being significant across groups of differential functional recovery."
  },
  {
    "objectID": "publications/2022-10-25-sci-human-proteomics/index.html#introduction",
    "href": "publications/2022-10-25-sci-human-proteomics/index.html#introduction",
    "title": "A comprehensive proteomic and bioinformatics analysis of human spinal cord injury plasma identifies proteins associated with the complement cascade and liver function as potential prognostic indicators of neurological outcome",
    "section": "",
    "text": "Spinal Cord Injury (SCI) is a major cause of disability, with complications post-injury often leading to life-long health issues with need of extensive treatment. Neurological outcome post-SCI can be variable and difficult to predict, particularly in incomplete injured patients. The identification of specific SCI biomarkers in blood, may be able to improve prognostics in the field. This study has utilised proteomic and bioinformatics methodologies to investigate differentially expressed proteins in plasma samples across human SCI cohorts with the aim of identifying prognostic biomarkers and biological pathway alterations that relate to neurological outcome."
  },
  {
    "objectID": "publications/2022-10-25-sci-human-proteomics/index.html#methods-and-materials",
    "href": "publications/2022-10-25-sci-human-proteomics/index.html#methods-and-materials",
    "title": "A comprehensive proteomic and bioinformatics analysis of human spinal cord injury plasma identifies proteins associated with the complement cascade and liver function as potential prognostic indicators of neurological outcome",
    "section": "",
    "text": "Blood samples were taken, following informed consent, from ASIA impairment scale (AIS) grade C ‚ÄúImprovers‚Äù (those who experienced an AIS grade improvement) and ‚ÄúNon-Improvers‚Äù (No AIS change), and AIS grade A and D at &lt;2 weeks (‚ÄúAcute‚Äù) and approx. 3 months (‚ÄúSub-acute‚Äù) post-injury. The total protein concentration from each sample was extracted, with pooled samples being labelled and non-pooled samples treated with ProteoMiner‚Ñ¢ beads. Samples were then analysed using two 4-plex isobaric tag for relative and absolute quantification (iTRAQ) analyses and a label-free experiment for comparison, before quantifying with mass spectrometry. Data are available via ProteomeXchange with identifiers PXD035025 and PXD035072 for the iTRAQ and label-free experiments respectively.\nProteomic datasets were analysed using OpenMS (version 2.6.0). R (version 4.1.4) and in particular, the R packages MSstats (version 4.0.1) and pathview (version 1.32.0) were used for downstream analysis. Proteins of interest identified from this analysis were further validated by enzyme-linked immunosorbent assay (ELISA)."
  },
  {
    "objectID": "publications/2022-10-25-sci-human-proteomics/index.html#results",
    "href": "publications/2022-10-25-sci-human-proteomics/index.html#results",
    "title": "A comprehensive proteomic and bioinformatics analysis of human spinal cord injury plasma identifies proteins associated with the complement cascade and liver function as potential prognostic indicators of neurological outcome",
    "section": "",
    "text": "The data demonstrated proteomic differences between the cohorts, with the results from the iTRAQ approach supporting those of the label-free analysis. A total of 79 and 87 differentially abundant proteins across AIS and longitudinal groups were identified from the iTRAQ and label-free analyses, respectively. Alpha-2-macroglobulin (A2M), retinol binding protein 4 (RBP4), serum amyloid A1 (SAA1), Peroxiredoxin 2, alipoprotein A1 (ApoA1) and several immunoglobulins were identified as biologically relevant and differentially abundant, with potential as individual prognostic biomarkers of neurological outcome. Bioinformatics analyses revealed that the majority of differentially abundant proteins were components of the complement cascade and most interacted directly with the liver."
  },
  {
    "objectID": "publications/2022-10-25-sci-human-proteomics/index.html#conclusions",
    "href": "publications/2022-10-25-sci-human-proteomics/index.html#conclusions",
    "title": "A comprehensive proteomic and bioinformatics analysis of human spinal cord injury plasma identifies proteins associated with the complement cascade and liver function as potential prognostic indicators of neurological outcome",
    "section": "",
    "text": "Many of the proteins of interest identified using proteomics were detected only in a single group and therefore have potential as a binary (present or absent) biomarkers, RBP4 and PRX-2 in particular. Additional investigations into the chronology of these proteins, and their levels in other tissues (cerebrospinal fluid in particular) are needed to better understand the underlying pathophysiology, including any potentially modifiable targets. Pathway analysis highlighted the complement cascasde as being significant across groups of differential functional recovery."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gabriel MateusBernardo Harrington, PhD",
    "section": "",
    "text": "Welcome! I‚Äôm a bioinformatician at the Dementia Research Institute at Cardiff University, working on understanding Alzheimer‚Äôs disease through data.\nLearn more about me ‚Üí\n\n    \n    \n  \n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Ramblings on academia",
    "section": "",
    "text": "Keyboard soldering adventures\n\n\n\nSoldering\n\nKeyboards\n\n\n\n\n\n\nJun 1, 2025\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Normality in R\n\n\n\nR\n\nDemo\n\nStatistics\n\n\n\n\n\n\nOct 21, 2022\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nProgress in open research!\n\n\n\nAcademia\n\nReproducible Research\n\nPublications\n\nJournals\n\nOpen Research\n\n\n\n\n\n\nJan 26, 2022\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nHow might science be done better?\n\n\n\nAcademia\n\nReproducible Research\n\nFantasising\n\n\n\n\n\n\nMay 11, 2021\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nThe Sovereign of Science\n\n\n\nAcademia\n\nReproducible Research\n\nRant\n\n\n\n\n\n\nMay 10, 2021\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Publication!\n\n\n\nAcademia\n\nPaper\n\nExplainer\n\n\n\n\n\n\nJul 27, 2020\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nTesting Out Blogdown\n\n\n\nDemo\n\nR Markdown\n\n\n\n\n\n\nJun 5, 2020\n\n\nMateus Harrington\n\n\n\n\n\nNo matching items\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2020-07-27-first-publication/index.html",
    "href": "blog/2020-07-27-first-publication/index.html",
    "title": "First Publication!",
    "section": "",
    "text": "So I finally got my first proper publication! The paper is open access now as well!\nProper here meaning I‚Äôm first author which, if you aren‚Äôt familiar with the fun(?) nuances of academia, is the only authorship that‚Äôs worth much (other than last author, which is also good).\nIt‚Äôs not that middle authorships are worthless.\nIt‚Äôs that they‚Äôre almost worthless.\nAnd to think, it only took just over a year (‚ï•Ôπè‚ï•)\nMy first actual authorship (which was a second authorship. Confused yet?) was for the paper this publication follows on from and that I briefly touch on in my previous post.\nBoth of these papers are about prognostic modelling of spinal cord injury. The rest of this post aims to summaries this work in a (hopefully) lay-friendly way, so even a non-academic can get the gist of things.\nI do this because my research, like the majority of science, is funded by the taxpayer, and so I feel the taxpayer should have access to the work for free and in a way that they can understand.\nAlso so I can continue my endless quest to procrastinate on my thesis, but that‚Äôs a tad less romantic so let‚Äôs not dwell on it."
  },
  {
    "objectID": "blog/2020-07-27-first-publication/index.html#happy-days",
    "href": "blog/2020-07-27-first-publication/index.html#happy-days",
    "title": "First Publication!",
    "section": "",
    "text": "So I finally got my first proper publication! The paper is open access now as well!\nProper here meaning I‚Äôm first author which, if you aren‚Äôt familiar with the fun(?) nuances of academia, is the only authorship that‚Äôs worth much (other than last author, which is also good).\nIt‚Äôs not that middle authorships are worthless.\nIt‚Äôs that they‚Äôre almost worthless.\nAnd to think, it only took just over a year (‚ï•Ôπè‚ï•)\nMy first actual authorship (which was a second authorship. Confused yet?) was for the paper this publication follows on from and that I briefly touch on in my previous post.\nBoth of these papers are about prognostic modelling of spinal cord injury. The rest of this post aims to summaries this work in a (hopefully) lay-friendly way, so even a non-academic can get the gist of things.\nI do this because my research, like the majority of science, is funded by the taxpayer, and so I feel the taxpayer should have access to the work for free and in a way that they can understand.\nAlso so I can continue my endless quest to procrastinate on my thesis, but that‚Äôs a tad less romantic so let‚Äôs not dwell on it."
  },
  {
    "objectID": "blog/2020-07-27-first-publication/index.html#so-whats-prognostic-modelling-anyway",
    "href": "blog/2020-07-27-first-publication/index.html#so-whats-prognostic-modelling-anyway",
    "title": "First Publication!",
    "section": "So what‚Äôs prognostic modelling anyway?",
    "text": "So what‚Äôs prognostic modelling anyway?\n\n\n\n\n\n\nFigure¬†1: Artist rendition of me pretending to work. Except not really. Image credit: iStock\n\n\n\nA prognostic model is something that aims to predict outcomes. In our context we‚Äôre trying to predict the neurological outcome (their motor and sensor function) for a patient following a spinal cord injury (SCI).\nCurrently it‚Äôs very difficult to know what sort of state a SCI patient will be in one year after injury for example. This is a problem for two main reasons.\nFirstly, it‚Äôs rather unpleasant for patients to be left with little idea as to what‚Äôs going to happen to them. They may ask questions like:\n\n‚ÄúWill I be able to walk again?‚Äù\n‚ÄúWill I be able to feed myself?‚Äù\n‚ÄúWhat about bathing, dressing and grooming?‚Äù\n\nTo which a clinician is often unable to give any answer beyond ‚ÄúNo‚Äù in the most severe SCI cases and ‚ÄúI don‚Äôt know‚Äù in other cases, which is the majority of the time.\n\n\n\n\n\n\nFigure¬†2: What does it even mean to ‚Äúknow‚Äù something in these crazy times anyway‚Ä¶? Image credit: Giphy\n\n\n\nThis uncertainty can end up making some patients unrealistically hopeful about their prospects, and others overly pessimistic, both of which can have a negative impact on mental health in both the long and short term.\nSecondly, as the level of spontaneous recovery varies wildly across patients, developing new therapies to treat SCI is extremely difficult. Imagine you have some fancy new therapy which you give to say 10 SCI patients. Half of them improve dramatically and nothing noteworthy happens to the other half.\nWas it your therapy that induced the improvement, or would those patients have recovered regardless of what you did or didn‚Äôt give them?\nDue to this, studies need very large sample sizes to account for the possibility of spontaneous recovery and prove a therapy is effective. This massively increases the costs of testing therapies which slows their progress.\nTherefore, this research aims to see if we can predict patient outcomes on admission to hospital."
  },
  {
    "objectID": "blog/2020-07-27-first-publication/index.html#okay-so-how-did-your-papers-go-about-attempting-this",
    "href": "blog/2020-07-27-first-publication/index.html#okay-so-how-did-your-papers-go-about-attempting-this",
    "title": "First Publication!",
    "section": "Okay, so how did your papers go about attempting this?",
    "text": "Okay, so how did your papers go about attempting this?\nRight, so without wishing to go into somewhat complex math, we took a bunch of data relevant to the patient injury, such as initial injury severity, age, diabetes status, etc., and built several models which aim to predict outcome scores for motor and sensor function at discharge and 12-months post-injury.\nImportantly, we also included any blood tests that were taken as part of routine care. To be clear, these are not blood tests that we scientists requested be done, but just done as part of normal care. We‚Äôre interested in routine bloods because we suspected they could provide valuable insight into outcomes at no additional cost to the healthcare provider, as the data had already been collected anyway.\nThis is in contrast to most of the other literature in this field, which focuses on cerebral spinal fluid (CSF). CSF would almost certainly have useful information for predicting SCI outcomes, but a very invasive procure is required to collect it, which introduces additional cost for the healthcare provider and risk to the patent.\n\n\n\n\n\n\nFigure¬†3: A nice cartoon of the spinal cord that illustrates which parts of the cord control which myotome (muscle groups). Image credit: Nature Reviews"
  },
  {
    "objectID": "blog/2020-07-27-first-publication/index.html#and-the-results",
    "href": "blog/2020-07-27-first-publication/index.html#and-the-results",
    "title": "First Publication!",
    "section": "And the results?",
    "text": "And the results?\nIn brief, it worked.\nI should stress at this point, for all my preamble, the main focus of the work at this stage is more to see if routine bloods can add predictive value for SCI outcomes rather than actual build a model that could be used in clinic.\nThe models we built were reasonably accurate at predicting outcomes, but to create something good enough to use in the clinic we would need an enormous sample size that goes way beyond the scope of these two papers. To give you a sense of scale, we used around 80 patients in our first preliminary paper and around 430 in this follow-up paper. We‚Äôd need many thousands if we wanted to propose the model be used in clinic.\nWe do hope to get there someday, but that‚Äôs certainly beyond the scope of my PhD so meh ¬Ø\\( Õ°‡≤†‚ÄØÕú ñ Õ°‡≤†)/¬Ø\nHowever, these papers provided evidence that routine blood markers can be somewhat predictive out outcome, which is what we were interested in.\nSo happy days!\n\n\n\n\n\n\nFigure¬†4: One happy looking rabbit. I wonder what drugs they gave it‚Ä¶ Image credit: Giphy\n\n\n\nBoth papers also suggest that several blood markers associated with liver function add value to predictions of outcome. This has peaked our interest and we hope to further investigate this link in subsequent work.\nMy next work will involve looking at SCI patient plasma with proteomics to see what differences there are in protein expression between people who experience significant neurological recovery as compared to those who don‚Äôt. Here‚Äôs hoping it doesn‚Äôt take another year to publish!"
  },
  {
    "objectID": "blog/2020-07-27-first-publication/index.html#feedback-welcome",
    "href": "blog/2020-07-27-first-publication/index.html#feedback-welcome",
    "title": "First Publication!",
    "section": "Feedback welcome!",
    "text": "Feedback welcome!\nSo I‚Äôm of the opinion that most academics are crap writters. This is often attributed to academics writing in a way that‚Äôs overly complex to sound smart.\nI think this is part of it, but personally I have been explicitly told to make my writing my complex, usually to cater to journals. The reality is that journals can often shun better written, more understandable prose and demand they be made ‚Äúfancier‚Äù.\nThis is just another reason why academic journals are the worst. Fair warning, don‚Äôt read that article if you‚Äôre having a good day, it‚Äôll ruin it.\nAs a result of this I really don‚Äôt have much of a gauge of how good I am at writing for a lay audience. So I welcome any feedback you might have about my writing as I‚Äôm keen to improve!\nThanks for reading, and have one final celebratory .gif for the road\n\n\n\n\n\n\nFigure¬†5: \\( Õ°&gt;‚ÄØ‚Äø‚Äø Õ°&lt;)/ Image credit: Giphy"
  },
  {
    "objectID": "blog/2020-06-05-testing-out-blogdown/index.html",
    "href": "blog/2020-06-05-testing-out-blogdown/index.html",
    "title": "Testing Out Blogdown",
    "section": "",
    "text": "Hello world!\nThis is my very first blog post using an R Markdown file within blogdown.\nThe three prior blog posts on this site came with the template, but I‚Äôm leaving them there for now as they are great introductory resources for using Academic.\nThis post (and this whole site!) wouldn‚Äôt be possible without the excellent teaching material from Dr.¬†Alison Hill, and the lovely people who made R and her many brilliant packages for which I offer all the gratitude I can muster!\nFor practice, I‚Äôm going to demonstrate how we can include the following in a single post:\n\nR code\nA plot\nAn image (via markdown syntax)\nAn image (via knitr::include_graphics)\nA relative link to another section in my site\nReading in data\n\n\n\n1) Iris data set\nLet‚Äôs make a plot using the iris data set in ggplot2.\n\n\nToggle the code\nlibrary(ggplot2)\n# Dataset\nknitr::kable(head(iris))\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n2) Plotting\n\n\nToggle the code\nscatter &lt;- ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width))\nscatter +\n  geom_point(aes(color = Species, shape = Species)) +\n  xlab(\"Sepal Length\") +\n  ylab(\"Sepal Width\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n3) Embedding a bundled image\nUsing markdown syntax ![Cute red panda](cute-red-panda-40.jpg) to generate the image below:\n\n\n\nCute red panda\n\n\n\n\n4) Using knitr\nWe can also accomplish this with a code chunk, setting the following parameter out.width=\"500px\", fig.align='center'\nknitr::include_graphics(\"cute-red-panda-40.jpg\")\n\n\n\nFigure 1: We can do nice auto-numbered captions as well! Image credit: Unsplash\n\n\n\n\n5) Relative links\nI can also include some relative links to, for example, tell you to check out this other section of my site!\nThe structure of relative links is a bit confusing. When specifying the location of the page you want to get to, the reference point is the directory that your post lives in (and NOT the project root directory).\n\n\n\n\n\n\nNote\n\n\n\nNow that I‚Äôve migrated my site to use Quarto instead of RMarkdown, the relative links now work how you‚Äôd expect, which is very nice!\n\n\n\n\n6) Reading in data\nLet‚Äôs flex our R muscles a little more by reading in some topical data, and making a fancier plot.\nThe data is total non-COVID-19 excess deaths per 100,000 and deaths involving COVID-19 per 100,000 for each 10-year age group from England and Wales, Week 11 to Week 18. The numbers around COVID-19 as in constant flux, so please be aware of the date. This data was downloaded from the Office of National Statistics on 2020-06-05.\n\n\nToggle the code\nlibrary(tidyverse)\ncovid_deaths &lt;- read_csv(\"covid_deaths_by_age_2020-06-05.csv\")\n# let's select the rows of actual data\ncovid_deaths_filtered &lt;- covid_deaths[7:16, ]\n# set column names\nnames(covid_deaths_filtered) &lt;- c(\n  \"age_group\",\n  \"non-covid_excess_mortality\",\n  \"covid_excess_mortality\"\n)\n# convert to long-format dataframe\ncovid_deaths_long &lt;- pivot_longer(\n  covid_deaths_filtered,\n  -age_group,\n  names_to = \"mortality_type\",\n  values_to = \"deaths\"\n)\n# convert deaths to a numeric\ncovid_deaths_long$deaths &lt;- as.numeric(covid_deaths_long$deaths)\n# plot\nggplot(covid_deaths_long, aes(x = age_group, y = deaths)) +\n  geom_bar(\n    aes(fill = mortality_type),\n    stat = \"identity\",\n    position = position_dodge()\n  ) +\n  ylab(\"Deaths registered per 100,000\") +\n  xlab(\"Age Group\") +\n  theme_bw() +\n  theme(legend.position = \"top\", legend.direction = \"horizontal\") +\n  scale_fill_manual(\n    name = \"\",\n    labels = c(\"COVID-19 mortality rate\", \"Non-COVID-19 excess mortality rate\"),\n    values = c(\"tomato\", \"steelblue\")\n  ) +\n  labs(caption = \"Source: Office for National Statistics, 2020-06-05\")\n\n\n\n\n\n\n\n\n\nIf you‚Äôre curious about explanations surrounding the increase in non-COVID-19 related deaths this article lists some theories.\nInterestingly, in the age groups up to 50 the non-COVID-19 death rate has declined slightly\n\n\nToggle the code\nknitr::kable(covid_deaths_filtered)\n\n\n\n\n\nage_group\nnon-covid_excess_mortality\ncovid_excess_mortality\n\n\n\n\n0 to 9\n-0.792423313\n0.014000412\n\n\n10 to 19\n-1.056672991\n0.130274752\n\n\n20 to 29\n-2.228613119\n0.653169144\n\n\n30 to 39\n-1.362056932\n1.944003908\n\n\n40 to 49\n-2.631069137\n6.697752064\n\n\n50 to 59\n4.289242454\n20.72845098\n\n\n60 to 69\n2.420093746\n55.40162727\n\n\n70 to 79\n46.66123612\n152.8772002\n\n\n80 to 89\n165.364023\n516.5874406\n\n\n90 and over\n752.7944396\n1190.274376\n\n\n\n\n\nAnd for a little context for these numbers, the Royal Society for the Prevention of Accidents reports that 30-60 people are stuck by lightning each year.\nThe Office for National Statistics estimated the population in the UK to be 66,796,807 in mid-2019. So if we take then high end of 60 lightning strikes, the odds of getting stuck by lightning in the UK is:\n\n\nToggle the code\n# stop scientific notation\noptions(scipen = 999)\n# calculate risk per 100,000\nrisk = (60 / 66796807) * 100000\n# print result\nprint(paste(\n  \"The incidence rate of lightning strikes per 100,000 = \",\n  round(risk, 5),\n  sep = \"\"\n))\n\n\n[1] \"The incidence rate of lightning strikes per 100,000 = 0.08982\"\n\n\nSo if we assume all humans are equally likely to get stuck by lightning in a given year, then it‚Äôs several times more likely for a 0-9 year old to get stuck by lightning than die with COVID-19.\nI should stress that this isn‚Äôt to make light of COVID-19, many people are dying, and no effort should be spared wherever this can be prevented. If anything, this should highlight how damaging COVID-19 is with a relatively low fatality rate. This pandemic could easily be even worse than it already is with more lethal pathogen behind it, and it is perfectly within the realm of possibility that the next pandemic, which is sadly inevitable, will star a deadlier disease. My greatest hope is that we can use this event as a wakeup call to stop ignoring the scientists who have warned this would happen repeatedly, and prepare better to mitigate future pandemics.\n\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "teaching/intro-to-r/index.html",
    "href": "teaching/intro-to-r/index.html",
    "title": "Introduction to R",
    "section": "",
    "text": "This lecture series is part of the Applied Bioinformatics and Genetic Epidemiology MSc course at Cardiff University.\nAs students come from both computational and life science backgrounds this module provides a basis for students to learn foundational bioinformatics skills, including Unix and R.\nThe aim of these R lectures is to leave students with the confidence to write their own R code as they move through the rest of the modules.\n\n\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "talk/2022-08-14-reproducible-data-analysis/index.html",
    "href": "talk/2022-08-14-reproducible-data-analysis/index.html",
    "title": "Reproducible data analysis",
    "section": "",
    "text": "This was a fun talk to give!\n\n\n\n Back to topReuseCC BY-SA 4.0"
  },
  {
    "objectID": "talk/index.html",
    "href": "talk/index.html",
    "title": "Presented Talks",
    "section": "",
    "text": "Proteomic and Bioinformatics Analyses of Plasma From Sci Neurological Improvers and Non Improvers\n\n\n\nPresentation\n\nAcademic\n\nProteomics\n\nSpinal Cord Injury\n\n\n\n\n\n\nOct 1, 2021\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible data analysis\n\n\n\nPresentation\n\nAcademic\n\nReproducible Research\n\n\n\n\n\n\nJun 15, 2021\n\n\nMateus Harrington\n\n\n\n\n\n\n\n\n\n\n\n\nJoint CDT Conference 2020\n\n\n\nPresentation\n\nPoster\n\nAcademic\n\nSpinal cord injury\n\nProteomics\n\n\n\n\n\n\nJun 8, 2020\n\n\nMateus Harrington\n\n\n\n\n\nNo matching items\n Back to topReuseCC BY-SA 4.0"
  }
]